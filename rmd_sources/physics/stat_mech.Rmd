---
title: "Statistical inference"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("ggplot2")) { install.packages('ggplot2', repos = "http://cran.us.r-project.org"); library("ggplot2") }
```

[Leonard Susskind: Stat Mech I](https://youtu.be/D1RzvXDXyqA?feature=shared)

> The second law of thermodynamics may be deeper and more general that a lot of other "flashy" physics.

Classical Mechanics, Quantum Mechanics, and other views, are about predictability; tracing a closed system by its rules of evolution from initial conditions onward.

Statistical mechanics allows you to make inferences without knowing the conditions or laws driving the thing. But it deals with bulk properties, not about tracking individual molecules, etc.

Good for elements

- too small to see
- too numerous to account for

## Probability

We want to think of the space of possibilities, whether outcomes of experiments, states of a system, etc, in a structured way.

So we define some rules:

|                                   |                       |
|-----------------------------------|-----------------------|
| $i \in 1...n$                     | number the states     |
| $P(i) \geq 0$                     | each state has some positive probability |
| $\sum_{i} P(i) = 1$               | summing over the probabilities gives $1$ |
| $\lim_{N \rightarrow \infty} N(i)/N = P(i)$ | as our number of experiments increases, we can approximate the probability of any state $P(i)$ |
|                                   |                       |

***

**Classic example: coin flips**

Through the *symmetry* of the perfectly fair coin, we say that:

$$
P(\text{heads}) = 0.5 \\
P(\text{tails}) = 0.5.
$$

In the absence of such symmetry, such as imperfectly machined coins, or weighted dice, we just don't know. We can measure $N(i)/N$ for a very large number of trials and we can get a close approximation, or we can appeal to the symmetries of some underlying theory to model the probability in question, but we ultimately can not know.

We can assign values to states of our system arbitrarily:

$$
F(\text{heads}) = -1 \\
F(\text{tails}) = 1,
$$

or these functions could be observables such as the energy or momentum.

We can take statistics on functions of these states, like the mean:


$$
\begin{align}
<F(i)> &= \sum_{i} F(i) P(i) \\
       &= \sum_{i} \frac{F(i) N(i)}{N}
\end{align}
$$

Note in this case that $<F(i)> = F(\text{heads}) P(\text{heads}) + F(\text{tails}) P(\text{tails}) = 0$, which is not a valid value for $F$, but is nonetheless a valid statistic.

***

## Symmetry

Continuing with symmetrical systems, suppose there is some law of motion describing the change from one state to the next, spending equal time in each state. In a small system we can just enumerate the states and their transitions as a finite state machine.

$$ 
\begin{align}
R &\rightarrow B \\
B &\rightarrow G \\
G &\rightarrow Y \\
Y &\rightarrow O \\
O &\rightarrow P \\
P &\rightarrow R
\end{align}
$$

We can still make observations on the evolution of these states, and without knowing the starting point or the detailed law, we can make a pretty good guess what a given snapshot will be.

A closed cycle of events for which you pass through each color once. If they are spending equal time on each color, over many trials we can simply say that $P = 1/6$.


Now if the states are not all connected:

```
  B
/   \
R - G

P - Y
\   /
  O
```

Without knowing which triangle we started in, we can still make progress. We know the probability of getting any one color in a given cycle is one third, and outside the cycle zero.

Though we couldn't find it through symmetry, suppose we know the probability for being in either cycle. From some outside tip or a strong hunch we *know* these probabilities.

Then, for the chance of getting any one color we take the probability that we're either in the top or bottom half, and then the probability within each cycle for our color:

$$
\begin{align}
P(\text{top}) \frac{1}{3} &= P(R|B|G) \\
P(\text{bottom}) \frac{1}{3} &= P(P|Y|O)
\end{align}
$$

This situation is what we call having a *conservation law*, where the underlying configuration space is broken up into cycles.

If we assigned a number to our states, $F(\text{top}) = -1, F(\text{bottom}) = 1$, this could be our conserved quantity; a number that, once observed in our closed system, never changes. It could be the *energy*, the *momentum*, or any suitable quantity deriving from the current state.

***

**Bad laws**:

- Have an irreversible end state: information is lost about the path taken
- States do not have exactly one entry and exit point: again it is not possible to reconstruct the path

They do not *conserve information*.

***

**Friction**

$$
\frac{\mathrm{d^2 x_n}}{\mathrm{d}t^2} = -\gamma \frac{\mathrm{d x_n}}{\mathrm{d}t}
$$

Seems to be a bad law. Motion grinding irreversibly to a halt.

What of [Liouville's theorem](https://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian))? The phase space distribution function is constant along trajectories of the system? It seems to violate energy conservation, and the second law of thermodynamics as well! 

Of course we know this can't be a closed system, and energy is being lost into the environment.

***

So lets say we know our system has only $M$ available states. We don't know how many states there possibly are in total, or how many may be in other cycles. We only know the states that are reachable; even so, we can describe it:

$$
\begin{align}
&N = \text{total} \\
\text{for} \space M < N, \space &P = 1/M \\
\text{others}, \space &P = 0
\end{align}
$$

So $M$ in relation to $N$ is a measure of our ignorance: when $M = N$ all states are equally likely and we essentially know nothing; when M = 1 we always know we're in the only available state.

This notion leads us to *entropy*, a quantity so fundamental that it shows up before energy and before temperature,

$$
S = \log M
$$

is conserved no matter how the configurations of the system change. It depends both on the state of the system and your state of knowledge of the system. It is a measure of the number of states that have a non-zero probability, and we will surely see it again.

***

**Continuous system**

Consider a continuous system of many particles. We draw a simplified phase space of the momenta and positions, $p$ vs $x$, keeping in mind that in general this is a multi-dimentional space, consisting of several degrees of freedom for each particle.

```{r echo=FALSE}
# For a $450\text{g}$ association football travelling near $150\text{km/h}$ around a $15\text{m}$ gym:
M <- 1000; m <- 0.450; v <- 150000*60*60; xmax <- 15
p0 <- rnorm(M, m*v*1.5, 30000000)
x0 <- rnorm(M, xmax/2, 1)
p1 <- rnorm(M, m*v/1.5, 85000000)
x1 <- rnorm(M, xmax/1.5, 1)

df <- data.frame(p = p0, x = x0, state="initial")
df[1001:2000, ] <- data.frame(p = p1, x = x1, state="later")
p1 <- ggplot(df, aes(x = x, y = p, colour = state)) + xlim(c(0, 15)) + ylim(c(0, 500000000))
p1 <- p1 + xlab("position (m)") + ylab("momentum (kg m/s)")
p1 <- p1 + geom_point(data = df, na.rm=TRUE)
p1
```

Each point here represents a state of our system. With a well formed law, we expect time to evolve each initial state to a single corresponding later state. The configurations may change, but there will always be $M$ non-zero probability states.

Working up toward an *infinite* number of states, in the fully continuous case, we can imagine a region representing reachable states, normally a multi-dimentional region in phase space. Appealing back to [Liouville's theorem](https://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)), we understand that the volume of that region does not change over its trajectories, nor does its distribution function. If all $M$ states were initially equally likely, we should continue to have *some* $M$ equally likely states.

***

[Orbits!](../index.html)
